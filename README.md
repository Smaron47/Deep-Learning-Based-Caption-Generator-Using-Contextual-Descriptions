# Deep-Learning-Based-Caption-Generator-Using-Contextual-Descriptions
his paper presents a deep learning-based caption generation model that utilizes structured textual features such as category, object presence, and spatial distance to produce descriptive captions. 
# Title: Deep Learning-Based Caption Generator Using Contextual Descriptions

## Abstract

This research introduces a deep learning-based textual caption generation model that leverages structured metadata rather than raw image data. Utilizing fields such as activity category, object identification, and spatial distance, the model is trained to generate accurate and contextually relevant textual descriptions. Developed using LSTM-based neural network architecture and trained on a custom dataset, the model targets applications in privacy-preserving environments, low-bandwidth settings, and automated documentation of human-centric activities. The system is implemented using TensorFlow and Keras, and is designed for both training and real-time inference using GPU acceleration on Google Colab.

---

## 1. Introduction

In recent years, automatic caption generation has gained significant attention in artificial intelligence and natural language processing. Traditional systems rely on visual inputs to derive meaningful descriptions. However, there exists a niche yet impactful requirement for generating such captions using structured textual data instead of images. This paper presents a framework where contextual fields such as activity category, observed objects, and their spatial measurements are encoded into a neural model that generates coherent and context-aware captions. This technique offers advantages in terms of speed, data privacy, and computational efficiency.

---

## 2. Dataset Preparation

### 2.1 Source

The dataset used in this study was created manually and stored in a CSV format. Each record in the dataset corresponds to a real-world event and includes:

* `category`: A high-level textual descriptor of the activity (e.g., "Brushing teeth").
* `objects`: List of present items, typically person(s), tools, or devices.
* `distance`: Spatial information capturing relative dimensions or object placements.
* `caption`: Ground-truth captions authored manually for training purposes.

### 2.2 Preprocessing

Data preprocessing involves eliminating rows with missing values in critical fields. A new feature column, `input_text`, is generated by concatenating `category`, `objects`, and `distance`, which becomes the primary input to the learning model.

```python
df['input_text'] = df['category'] + ", " + df['objects'] + ", " + df['distance']
```

---

## 3. Methodology

### 3.1 Text Tokenization

To process textual data, we employ TensorFlowâ€™s `Tokenizer` class. It converts words to numerical indices and ensures uniform sequence length via padding. A vocabulary cap of 5000 words is used to maintain computational feasibility.

```python
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(input_texts + captions)
```

### 3.2 Model Architecture

The model is constructed using a sequential architecture, designed for simplicity and performance:

* **Embedding Layer**: Translates token indices into dense vector representations.
* **LSTM Layer**: Learns sequential dependencies and context.
* **Dropout Layers**: Introduced for regularization to mitigate overfitting.
* **Dense Layers**: Produce final predictions across the entire vocabulary using softmax activation.

```python
model = Sequential([
    Embedding(vocab_size, 64, input_length=max_seq_len),
    LSTM(32, return_sequences=False),
    Dropout(0.2),
    Dense(256, activation='relu'),
    Dropout(0.2),
    Dense(vocab_size, activation='softmax')
])
```

The model is compiled using the Adam optimizer and `sparse_categorical_crossentropy` loss, making it suitable for classification over a large vocabulary.

---

## 4. Training

The training process consists of 100 epochs with a small batch size of 4 to accommodate GPU memory constraints. The target is the first word of the caption, simplifying the problem into a single-token prediction. Although this approach limits output complexity, it ensures fast convergence and high word accuracy. Future improvements may include sequence-to-sequence training with teacher forcing.

```python
model.fit(input_padded, np.expand_dims(caption_padded[:, 0], axis=-1), epochs=100, batch_size=4, validation_split=0.2)
```

The trained model is saved in `.h5` format to Google Drive, enabling easy reuse and deployment.

---

## 5. Evaluation and Inference

### 5.1 Inference Pipeline

The model is evaluated using unseen data rows. Each row undergoes the same preprocessing and tokenization pipeline before being input to the model. Examples of test input include varied real-world contexts, such as:

```python
test_inputs = [
    "Attending a meeting or class (POV at a desk or table), 3 person, 0.64m, 0.64m, 2.39m",
    "Attending a meeting or class (POV at a desk or table), 2 person, 1 laptop, 0.58m, 0.62m, 2.27m",
    "Brushing teeth (POV looking in the mirror),1 person,0.54m",
    "Charging a smartphone (POV with a charger and port visible),no objects,0m"
]
```

### 5.2 Caption Synthesis

After the prediction, word indices are mapped back to words using a reverse tokenizer. To enhance readability, the results are grouped into chunks of two lines.

```python
captions = [reverse_word_index.get(idx, "<Unknown>") for idx in predicted_indices]
combined_caption = "\n".join([" ".join(captions[i:i + 2]) for i in range(0, len(captions), 2)])
```

---

## 6. Results

The model demonstrates consistent ability to generate logical, if simplified, single-word captions grounded in the structured context. This validates the feasibility of using structured data alone for captioning tasks. Performance can be quantitatively improved by expanding training targets to full sentences and incorporating recurrent decoder layers or attention modules.

---

## 7. Deployment Considerations

* **Compute Environment**: Training and inference require GPU support, verified at runtime.
* **Data Accessibility**: All inputs and outputs are synchronized with Google Drive for persistent storage.
* **Scalability**: The model architecture supports easy scaling by adjusting vocabulary, sequence length, or network depth.
* **Integration Potential**: Can be deployed in assistive applications, content summarization tools, or smart surveillance systems.

---

## 8. Conclusion and Future Work

This study successfully establishes a novel approach to textual caption generation based on structured metadata rather than visual inputs. It opens avenues for efficient and private AI applications that interpret user activities or object contexts through non-visual cues. Planned future enhancements include:

* Multi-word caption generation using sequence models
* Integration of attention mechanisms for contextual relevance
* Evaluation using BLEU or ROUGE metrics
* Expansion to multilingual datasets and domains

---

## References

* Keras Documentation: [https://keras.io/](https://keras.io/)
* TensorFlow Documentation: [https://www.tensorflow.org/api\_docs](https://www.tensorflow.org/api_docs)
* Colab Guide: [https://research.google.com/colaboratory/faq.html](https://research.google.com/colaboratory/faq.html)
* Bahdanau et al., 2014. "Neural Machine Translation by Jointly Learning to Align and Translate"
* Sutskever et al., 2014. "Sequence to Sequence Learning with Neural Networks"
